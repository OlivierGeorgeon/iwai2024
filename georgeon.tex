% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\begin{document}
%
\title{Reducing Intuitive-Physics Prediction Error through Playing}
%
\titlerunning{Reducing Prediction Error through Playing}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Olivier L. Georgeon\inst{1, 2}\orcidID{0000-0003-4883-8702} \and
Paul Robertson\inst{3}\orcidID{0000-0002-4477-0379} }
%
\authorrunning{Georgeon and Robertson}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{UR CONFLUENCE: Sciences et Humanites (EA 1598), UCLy, France 
	\email{ogeorgeon@univ-catholyon.fr} \and
SyCoSMA, LIRIS, CNRS, Villeurbanne, France \and
DOLL Labs, Lexington, MA, USA\\
\email{paulr@dollabs.com}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
We present a method for an autonomous robot to generate behaviors to calibrate its intuitive-physics engine also known as the ``Game Engine in the Head'' (GEITH).
%The robot selects behaviors that yield information to refine the GEITH parameters. 
At the beginning of each interaction cycle, the robot uses its GEITH to run a simulation to compute predicted sensory signals. 
For each sensor, prediction error is the difference of the predicted sensory signal minus the actual sensory signal received at the end of the interaction cycle. 
Results show that over a few tens of interaction cycles, the robot improves its GEITH calibration and decreases its prediction errors. 
Moreover, the robot generates behaviors that human observers describe as playful.

\keywords{Active infernce  \and constructivist learning \and enaction \and intrinsic motivation \and robotics \and core knowledge.}
\end{abstract}
%
%
%
\section{Introduction}

It is widely believed that cognitive beings possess some kind of a \textit{world model} that they use to generate intelligent behaviors.
How they construct, maintain, and use this world model remains, however,  an open question in cognitive science and artificial intelligence. 

Karl Friston and his research group have proposed Active Inference \cite[e.g.]{smith_step-by-step_2022} as a method to infer the world model by minimizing \textit{free energy} \cite{friston_free-energy_2010}.
The world model at step $t$ is represented as a probability distribution $\mu_t$ that give the probability of being in any world state $s \in S$. 
This method iteratively updates $\mu_t$ on each interaction cycle.
The agent selects the action that is expected to maximize the information gained. 
The estimation of expected information gained is computed through the variational free energy which involves a divergence between two probability distributions: the agent's world model $\mu$ and the joint probability distribution $g = P(O, S)$ of observations $O$ and world states $S$ called the \textit{generative model}. 
This method, however, requires that the set of states $S$ and the relations between states and observations $O$ be known \textit{a prior}. 
Moreover, the high computational requirements to compute the free energy and the high number of interaction cycles to converge to a useful world model makes this method inapplicable in our case of a robot interacting with the open world. 

The Partially Observable Markov Decision Process (POMDP) literature proposes a broad range of methods to infer a \textit{belief state} in a partially observable process.
The belief state amounts to the agent's world model of the environment that the agent can only partially observe.  
If the state transition function and the observation function are known \textit{a priori}, the problem of computing the belief state has been mathematically solved \cite{astrom1965optimal}. 
It was also proven that the implementation of the solution becomes intractable as the set of states and observation grows. 
Without knowledge of the state transition and observation functions, the problem of inferring belief states in POMDPs does not lend itself to a mathematical analysis. 

The active inference and the POMDP literature led us to hypothesize that inferring the world model through experience of interaction requires prior assumptions to reduce complexity. 
The present study examines how the ``Game Engine In The head'' (GEITH) can work as a suitable prior assumption that an autonomous robot could use to reduce its prediction errors. 

Joshua Tenenbaum and his research group have proposed the GEITH \cite{battaglia_simulation_2013} as the capacity of cognitive beings to simulate basic dynamics of physics and interactions. 
In mammals, the GEITH would rest upon brain structures that are partially predefined by genes and then completed through ontogenetic development.  
Similarly, it is possible to endow artificial agents and robots with a predefined software game engine, and expect them to refine the parameters of their game engine as they test their predictions in the world.

The refinement of the game engine is measured through two methods. 
The first is performed by the robot itself by measuring the prediction error of sensory signals. 
Decrease in prediction errors shows improvement of the game engine. 
The second is performed by the experimenter by assessing whether the game engine parameters converge towards a target range that indicates that the robot managed to calibrate its GEITH. 




\section{Our hypothesis}

We comply with active inference theory in several regards. 
Firstly, we do not consider sensory signals to be \textit{representational} of the environment's state. 
Different sensory signals may come out of the same environment state depending on the agent's action. 
This implies a ``conceptual inversion'' of the interaction cycle in which action comes first and sensory signal comes second as an  \textit{outcome} of action. 
Secondly, we give no presupposed ontological knowledge about entities in the environment to the agent. 
The agent must infer the existence and properties of entities by itself through patterns of interactive experience. 
This view can be tracked back to Whitehead's process philosophy in which phenomenal experience involves abstracting entities out of events \cite{whitehead1929}. 
Thirdly, no extrinsic goal is encoded in the agent in the form of goal states that the agent should search based on reward or other criteria. 
We nonetheless accept to encode some \textit{prior preference distribution} on tuples $\langle$action, outcome$\rangle$ that encode ``innate'' preferences of interactions. 
In short, the agent has no \textit{rewarding world states} but has \textit{rewarding interactions} (positive or negative).
For a deeper examination of these principles in the active inference literature, we refer the reader to our previous article \cite{georgeon_artificial_2024}.

We also comply with the idea that \textit{prediction error} is a good measure of the quality of the agent's world model, and minimizing surprise is a powerful motivator. 
Indeed, to survive, cognitive beings should generally stay away from bad surprises.
We, however, found that learning through gradient descent of prediction error (or surprise) was not applicable to our case of a robot interacting with an open world. 
We are investigating another approach based on an innate set of playful behaviors that the robot may select depending on its \textit{emotional state}.  
In our case, prediction-error reduction is not a means to improve the world model but a consequence.

We use Hugo Lövheim's ``cube of emotions'' \cite{lovheim_new_2012} as a basic emotional model based on three monoamines: dopamine (DA), serotonin (5-HT), and nor-adrenaline (NA) (Fig. \ref{fig:geith}, top right).
This model associates dopamine with pleasure and reward-seeking behavior, serotonin with well-being and playful behavior, and nor-adrenaline with arousal and stress responses.
It has been successfully used for simple emotional robotics.
For example, Max Talanov and his team generated basic emotional behaviors easily interpretable by human observers \cite{chebotareva_emotional_2019}. 
They showed that the observer's understanding of the robot's behavior was improved when the robot displayed its internal emotional state through an intuitive color code related to the prevalence of one of the three neurotransmitters, green: dopamine, white: serotonin, red: nor-adrenaline, and blue when all three neurotransmitter levels were low. 

The cognitive architecture is seeded with innate behaviors. 
This study does not examine the learning of new sequences of behaviors (this was the object of other studies \cite{georgeon_cash_2019}) but only the learning of parameters. 

We investigate the core knowledge system \cite{spelke_core_2012}.

The robot must follow a chain of causality to explain the prediction errors \cite{thorisson_explanation_2021}.

\begin{figure}
	\includegraphics[width=\textwidth]{Figure_geith.pdf}
	\caption{The game engine within the robot's cognitive architecture.
	Bottom: the history of interactions enacted over time.
	Center: the working memory that implements the game engine and proposes future behaviors.
	Left: the types of phenomena inferred trough interactive experience.
	Top center: set of behaviors.
	Top right: The three-dimensional emotional state based on dopamine (DA), serotonin (5-HT), and nor-adrenaline (NA) levels.
	Right: the decider selects the next behavior based on the emotional state and the expected outcome predicted from simulation.} \label{fig:geith}
\end{figure}



\section{Experiment}

We designed a robotic platform called Petitcat based on the ``robot car'' commercialized by Osoyoo \cite{osoyoo_robot_car}.
For this experiment, we added an Inertial Measurement Unit (IMU) and an RGB LED as the emotion indicator (Fig. \ref{fig:video}). 

The C++ software running on the Arduino board controls the enaction of the sensorimotor loops. 
A personal computer implements the GEITH and the cognitive architecture that remote controls the robot through wifi.
The code is open source and shared online \cite{petitcat_github}.

The levels of all three neurotransmitters can vary from 0 to 100 and are initialized at 50. DA prevails when DA = 5-HT = NA.
Prevalence of DA makes Petitcat initially select the \texttt{move forward} behavior that is intrinsically rewarding.
When he encounters a new object, 5-HT increases to MAX-5-HT. Prevalence of 5-HT triggers the selection of playful interactions with this object.
On each  \texttt{play} interaction, if the prediction errors do not decrease (i.e., prediction does not improve), 5-HT decreases by 1.
When 5-HT decreases below or equal to DA, exploration behaviors are again selected meaning that Petitcat disinterests from the object and goes exploring new destinations. 
During play, when Petitcat unexpectedly fails to detect the object, NA increases to MAX-NA, which causes the selection of \texttt{search} behaviors. 
DA then decreases of 1 on each interaction or is reset to MIN-NA when Petitcat finds the object again.  


\begin{figure}
	\includegraphics[width=\textwidth]{Figure_video.pdf}
	\caption{Screenshot of a video example run \cite{georgeon_petitcat_2024}.
	Left: Petitcat playing with a black dot on the floor.
	Top right: Petitcat's egocentric memory. Black segments: black dot detection events. 
	Bottom right: allocentric memory. Black hexagon: the black dot used as a point of reference. Yellow hexagons: echo measured with the sonar. Red hexagon: the robot's focus of attention.} \label{fig:video}
\end{figure}


% \subsection{The robotics platform}

% \subsection{The Game Engine In The Head}

\section{Results}

Several videos are available online. Here we analyze \cite{georgeon_petitcat_2024}.

\begin{figure}
	\includegraphics[width=\textwidth]{01_Outcome_code.pdf}
	\caption{Prediction error of black line detection.
	Step 2: the robot did not expect to detect the dot.
	Steps 17: the robot expected to detect the dot while translating forward but missed it.
	Steps 18: the robot expected to not detect the dot while turning around but detected it.
	Step 20: the robot did not predict detecting the dot through simulation but it did. 
	Step 65: The robot did not expect to detect the surrounding arena. } \label{fig:yaw_pe}
\end{figure}

\begin{figure}
	\includegraphics[width=\textwidth]{07_Forward_duration_pe.pdf}
	\caption{Move forward duration prediction error.
	Step 2 and 20: the forward translation was unexpectedly interrupted by the dot detection.
	Step 17: the forward duration was longer than expected because the robot did not detect the dot.
	From Step 21 to 64: forward duration prediction error slightly decreases. 
	Step 65: the forward translation was unexpectedly interrupted by the detection of the arena border.} \label{fig:yaw_re}
\end{figure}


\begin{figure}
	\includegraphics[width=\textwidth]{02_yaw_pe.pdf}
	\caption{The yaw prediction error shows no significant trend when we do not distinguish between the different kinds of interactions.} \label{fig:yaw_pe}
\end{figure}

\begin{figure}
	\includegraphics[width=\textwidth]{03_yaw_re.pdf}
	\caption{The yaw residual error during interactions in which the robot detects the dot decreases significantly after Step 26.} \label{fig:yaw_re}
\end{figure}

\begin{figure}
	\includegraphics[width=\textwidth]{04_Compass.pdf}
	\caption{The compass residual error decreases after step 20 when the robot starts circling around the dot.
	It nonetheless remains noisy due to sensor imprecision.
	The sliding average over 10 interactions tends to 0.8° and the standard deviation to 7.4°.} \label{fig:compass}
\end{figure}

\section{Conclusion}

We are not claiming the robot can actually \textit{experience} emotions let alone have sentience. 
His internal model of emotions, nonetheless, helps generate behaviors that human observers easily interpret as lifelike. 
The emotional indicator facilitates this interpretation. 

\begin{credits}

%\subsubsection{\ackname} A bold run-in heading in small font size at the end of the paper is
%used for general acknowledgments, for example: This study was funded
%by X (grant number Y).

%\subsubsection{\discintname}
%It is now necessary to declare any competing interests or to specifically
%state that the authors have no competing interests. Please place the
%statement with a bold run-in heading in small font size beneath the
%(optional) acknowledgments\footnote{If EquinOCS, our proceedings submission
%system, is used, then the disclaimer can be provided directly in the system.},
%for example: The authors have no competing interests to declare that are
%relevant to the content of this article. Or: Author A has received research
%grants from Company W. Author B has received a speaker honorarium from
%Company X and owns stock in Company Y. Author C is a member of committee Z.
\end{credits}
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{georgeon.bib}
%
\end{document}
