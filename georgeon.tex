% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%

% Added by olivier
\usepackage{url}
\def\UrlBreaks{\do\/\do-}

\begin{document}
%
\title{Reducing Intuitive-Physics Prediction Error through Playing}
%
\titlerunning{Reducing Prediction Error through Playing}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%

\author{Olivier L. Georgeon\inst{1, 2}\orcidID{0000-0003-4883-8702}  \and Béatrice de Montéra \inst{1, 3}\orcidID{0000-0002-3579-4870} \and Paul Robertson\inst{4}\orcidID{0000-0002-4477-0379}}
\authorrunning{Georgeon, de Montéra, and Robertson}

% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.

\institute{UR CONFLUENCE: Sciences et Humanites (EA 1598), UCLy, France \email{ogeorgeon@univ-catholyon.fr}\\ \email{bdemontera@univ-catholyon.fr}  
	\and SyCoSMA, LIRIS, CNRS, Villeurbanne, France 
	\and MetaGenoPolis Unit, INRAE, Jouy-en-Josas, France \\
	\and DOLL Labs, Lexington, MA, USA\\ \email{paulr@dollabs.com}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
We present a mobile robot that autonomously generates behaviors to calibrate its intuitive-physics engine, also known as the ``Game Engine in the Head'' (GEITH).
Most POMDP and Active Inference learning techniques operate in a closed world in which the set of states is defined a priori. 
However, implementing an ``innate'' GEITH and a set of interactive behaviors allowed us to avoid these limitations and design a mechanism for information search and learning in an open world. 
The results show that over a few tens of interaction cycles, the robot's prediction errors decrease, which shows an improvement in the GEITH calibration. 
Moreover, the robot generates behaviors that human observers describe as playful.

\keywords{Active inference  \and developmental learning \and enaction \and intrinsic motivation \and robotics \and core knowledge.}
\end{abstract}
%
%
%
\section{Introduction}
\label{sec:intro}

%Cognitive beings are widely believed to possess some kind of actionable \textit{world model} that they use to generate intelligent behaviors.
%The development of this world model may begin during an early stage of life through basic physical inter\-actions---a stage called ``development of sensorimotor structures'' by psychologists such as Piaget \cite[p. 104]{dolle_pour_2005}. 
%However, transferring these ideas into artificial intelligence and robotics remains an open challenge.

In their book ``The pragmatic Turn'', Engel, Friston, and Kragic \cite{engel_pragmatic_2015} advocate a shift from a representation-centered perspective to an ``action-oriented'' perspective on cognition.  
Aligned with this shift, we present a robotics implementation to study the intricacy between action and perception.
Closely related to pragmatic philosophy, Whitehead's process philosophy proposes useful concepts to describe perception from entities 
having unconscious experience of connection to the world,
%that may not be conscious 
which he calls \textit{enduring objects} \cite{whitehead1929}. 
% He gathers these entities in a broad category called \textit{enduring objects} \cite{whitehead1929}. 
He distinguishes between two modes of perception: the mode of \textit{presentational immediacy} and the mode of \textit{causal efficiency} \cite{smith_social_2010}. 
We apply these concepts to robotics: sensory input is provided to the robot in the mode of presentational immediacy, and the robot learns spatio-sequential patterns of actions and outcomes that account for the causal-efficiency mode of perception. 
%The robot refines an actionable causal model from interactive experience to interact more efficiently with entities found in the world.  

%The Partially Observable Markov Decision Process (POMDP) literature has proposed a broad range of methods to infer a \textit{belief state} in a partially observable process.
%The agent uses the belief state as a model of the environment that the agent observes through partial observations.  
%If the POMDP's observation and transition functions are known \textit{a priori}, mathematical formulae to optimally compute the belief state have been established but proved to be generally intractable %(NP-Hard) \cite{kaelbling1998planning}. 

Karl Friston and his research group have proposed Active Inference as a method to interactively learn the causes of sensory signals by minimizing \textit{free energy}, or equivalently, prediction error \cite{friston_free-energy_2010}.
Active inference has been used in the framework of Partially Observable Markov Decision Processes (POMDPs) to allow artificial agents to learn a causal model of the environment that they can only partially observe \cite{smith_step-by-step_2022}. 
The causal model uses the distribution of the probability of each possible state of the world. 
In essence, at each instant, the agent estimates which states are the most or least likely to be the actual state of the world.
The \textit{expected information gained} to enhance this estimate is involved when selecting the next action.
Active inference has been used in robotics \cite{lanillos_active_2021} but generally under \textit{closed-world} settings in the sense that the set of possible states is known \textit{a priori}---a requirement for most of the mathematical apparatus of active inference. 

When the robot is thrown into an open world, the problem of learning the cause of sensory input remains open. 
The POMDP and active inference literature suggest that the robot needs prior assumptions about the world to cope with complexity \cite{georgeon_artificial_2024}. 
The present study examines how the ``Game Engine In The head'' (GEITH) can work as a suitable prior assumption that an autonomous robot could use to maintain a causal model of perception in the open world and reduce prediction errors. 

Joshua Tenenbaum and his research group have proposed GEITH \cite{ullman_mind_2017} as the capacity of cognitive beings to simulate the basic dynamics of physics and interactions. 
In the brain, the GEITH rests on structures that are partially predefined by genes and then completed through ontogenetic development.  
Similarly, it is possible to endow artificial agents and robots with a predefined software game engine and expect them to refine the parameters of their game engine and modify their predictions through ongoing interaction.
The refinement of the game engine is assessed through the prediction error of sensory signals. 
The decrease in prediction errors shows an improvement of the game engine. 

Compared with general Bayesian models classically used in active inference, GEITH adds the assumption that all the sensorimotor experiences can be localized in the 3D Euclidean space, at least approximately. 
It implements predefined linear-algebra functions to compute spatial transformations (rotations and translations) between frames of reference.


\section{Our hypothesis}

We comply with active inference theory in several regards. 
Firstly, we do not assume that sensory signals are \textit{representational} of the state of the world. 
%Different sensory signals may come out of the same environment state depending on the agent's action. 
% Each state of the world can generate different sensory signals depending on the action performed by the agent.
The world is hidden to the agent so that a given state may return contrary sensory signals when acted on differently by the agent.
This implies a ``conceptual inversion'' of the interaction cycle in which action comes first and the sensory signal comes second as an  \textit{outcome} of action. 
Secondly, we do not provide the agent with presupposed ontological knowledge about entities in the world. 
The agent must infer the 
%existence and properties of entities by itself 
presence of \textit{causes} in the world 
through patterns of interactive experience. 
%This view can be traced back to Whitehead's process philosophy in which phenomenal experience involves abstracting entities out of events \cite{whitehead1929}. 
Thirdly, no extrinsic goal is encoded in the agent in the form of goal states that the agent should search based on reward or other criteria. 
However, we may associate some \textit{prior preference} with interactions. 
In short, the agent has no \textit{rewarding world states} but has \textit{rewarding interactions}. % (positive or negative).
For a deeper examination of these principles in relation to the active inference literature, we refer the reader to 
%our previous article 
\cite{georgeon_artificial_2024}.

We also adopt \textit{prediction error} as a measure of the quality of the agent's world model. %, and minimizing surprise is a powerful motivator. 
%Indeed, to survive, cognitive beings should avoid bad predictions as much as they can. %generally stay away from bad surprises.
%As introduced in Section \ref{sec:intro}, 
However, we are not using the gradient descent of the prediction error as a motivational principle to drive the learning process. 
As we develop, our agent is not always driven by a value optimization process; it may also enact behaviors that we call \textit{disinterested}.
Disinterested behaviors can consist of aimless innate tendencies, or individual habits taken through the agent's lifetime.
% We are investigating an alternative approach based on an innate set of behaviors that the robot may select depending on its \textit{emotional state}.  
In this implementation, the reduction of prediction errors is not a means of improving the world model, but a consequence of its improvement.

We are using a cognitive architecture 
%that we 
designed previously 
based on sensorimotor and enactive principles \cite{georgeon_artificial_2024}. 
The present article reports the integration of the new GEITH module within this cognitive architecture, as illustrated in Figure \ref{fig:geith}. 
The GEITH supports the simulation of behaviors before their selection by the cognitive architecture and their enaction by the robot. 
At the beginning of each interaction cycle, the simulation computes the \textit{predicted outcome}.
At the end of the interaction cycle, the predicted outcome is compared with the \textit{actual outcome} to calculate the prediction error.   
We investigate the core elements of the GEITH that are needed for the agent to reduce prediction error. 

We draw inspiration from studies on \textit{core knowledge} in the brains of animals and human infants. 
For example, Elizabeth Spelke and her colleagues argued for the existence of two core geometry systems that ``evolved before the emergence of the human species'':  
``The \textit{core navigation system} captures absolute distance and sense [..] but not relative length or angle; the \textit{core form analysis system} does the reverse'' \cite[p. 2789]{spelke_core_2012}.
We start by implementing the minimal requirements she deems necessary for both systems, namely the ability to handle points in spatial memory, the foundational elements of Euclidean geometry. 

Our cognitive architecture encodes behaviors as \textit{composite interactions} which are sequences of \textit{primitive interactions}.
A primitive interaction is a \textit{control loop} that involves actuator commands, expected sensory feedback, spatio-temporal attributes, termination conditions, termination outcome, and prior preference. 
Examples are given in Section \ref{sec:expe}. 
 GEITH may consider some of the outcomes as the result of interaction with ``something'' in the environment.
In this case, the GEITH instantiates a data structure called a \textit{phenomenon}\footnote{Common-sense usage of the term \textit{phenomenon}: ``something'' that a cognitive being perceives in the environment. Technically: ``any useful grouping of a subset of spatio-temporal patterns experienced by an agent in an environment'' \cite[p. 8]{thorisson_explanation_2021}.} and localizes this phenomenon at the position of the interaction in spatio-sequential memory.
Next, GEITH simulates subsequent interactions with phenomena to predict future outcomes. 
%The present study focuses on the simplest possible kind of phenomenon: points on the two-dimensional floor.

We seeded the cognitive architecture with ``innate'' composite interactions that cause the robot to explore the environment and interact with points encountered on the floor (Fig. \ref{fig:geith}, top center). 
In other studies, we implemented the learning of new composite interactions \cite{georgeon_cash_2019}, but here we only examine the refinement of the GEITH parameters allowing the tuning of primitive interactions to reduce the prediction error.

\begin{figure}
	\includegraphics[width=\textwidth]{Figure_geith.pdf}
	\caption{The game engine within the cognitive architecture (derived from \cite{georgeon_artificial_2024}, Fig. 6).
		Bottom: the history of interactions enacted over time.
		Rightward triangles: \texttt{forward}. Leftward triangles: \texttt{backward}. Squares: \texttt{swipe}. Circles: \texttt{turn}. shade: outcome \texttt{white} or \texttt{black}.
		Center: the GEITH.
		Red hexagon: the focus of attention localized at the position of the phenomenon. 
		Magenta hexagon: the prompt is the localization of the next selected interaction's destination: \texttt{swipe} to the right.
		Left: the types of phenomena inferred through interactive experience.
		Solid objects, walls, and other robots can be detected by the echo-localization sensor, but are not present in this experiment.
		Top center: predefined composite interactions and GEITH parameters.
		Top right: three-dimensional emotional state based on dopamine (DA), serotonin (5-HT), and nor-adrenaline (NA).
		Right: the cognitive architecture selects the next behavior based on the emotional state and the expected outcome predicted by the GEITH.} \label{fig:geith}
\end{figure}

The cognitive architecture uses variables that represent the robot's \textit{emotional state} to select composite interactions to try to enact.
We use Hugo Lövheim's ``cube of emotions'' \cite{lovheim_new_2012} as a basic emotional model based on three neurotransmitters: dopamine (DA), serotonin (5-HT), and nor-adrenaline (NA) (Fig. \ref{fig:geith}, top right).
This model associates dopamine with pleasure and reward seeking behavior, serotonin with well-being and playful behavior, and nor-adrenaline with responses to arousal and stress.
It has been used successfully for simple emotional robotics.
Our robot visually indicates its predominant neurotransmitter level using an intuitive color code studied by Max Talanov and his team: green for dopamine, white for serotonin, red for noradrenaline, and blue when all three neurotransmitter levels are low \cite{chebotareva_emotional_2019}.

The GEITH implements two levels of spatio-sequential working memory: \textit{egocentric} and \textit{allo-phenomenon-centric} (Fig. \ref{fig:geith}, center).  
The interactions and displacements are received in the egocentric reference frame based on the position of sensors and translation speed given as GEITH parameters, and the yaw measured by the Inertial Measurement Unit (IMU) which plays a similar role as the vestibular system (Fig. \ref{fig:video}, top right).
When the robot encounters a new phenomenon, the GEITH instantiates a new allocentric reference frame centered on this phenomenon to track the displacement of the robot relative to this phenomenon (Fig. \ref{fig:video}, bottom right). 
Comparable mechanisms of coordinate conversion have recently been found in the brain \cite{wang_egocentric_2020} and implemented in other software cognitive architectures \cite{schneider_emergence_2024}. 
Phenomenon-centric frames relate to Jeff Hawkins' thousand brain hypothesis \cite{hawkins_framework_2019}, according to which the brain records thousands of small spatio-temporal models to memorize interactions with different kinds of object.

%We implemented Tenenbaum's idea to give a \textit{sleep}/\textit{awake} attribute to phenomena. If the GEITH detects that the phenomenon moves, it switches the attribute to \textit{awake}.

Once the robot has selected an object in the environment, its serotonin level increases, which triggers behaviors of interaction with this object to calibrate its GEITH parameters. 
Phenomenon-centric memory is discretized into a hexagonal grid inspired by grid cells in the entorhinal cortex \cite{moser_place_2008}. 
The cognitive architecture uses this grid as a small finite discrete model in which to search for information and optimize it. % with an algorithm off the shelf. 


\section{Experiment}
\label{sec:expe}

We designed a mobile robotic platform called Petitcat\footnote{Sections \ref{sec:expe} and \ref{sec:results} personalize the robot by name and pronoun to enhance readability. We do not claim that he has a psychology or gender.} based on the \textit{Osoyoo robot car} \cite{osoyoo_robot_car}.
The experiment reported here uses only two sensors.
The \textit{floor luminosity sensor} is a bar of 5 infrared-reflective sensors directed to the floor.
From this bar of sensors, we retrieve 4 possible signals:  \texttt{none},  \texttt{left},  \texttt{front}, or \texttt{right} signaling the absence or relative position of a black tape present beneath them.  
The IMU measures the yaw during the enaction of interactions.
Note that Petitcat cannot see the black tape from a distance. 
He has no camera, lidar, or odometer.
What looks like eyes on his head is an ultrasonic echo-localization sensor not exploited in this experiment. 
The emotion indicator is an RGB LED (Fig. \ref{fig:video}). 

\begin{figure}
	\includegraphics[width=\textwidth]{Figure2_video.pdf}
	\caption{Screenshot of a video example run \cite{georgeon_petitcat_2024}.
		Left: Petitcat playing with a point made of a piece of black tape on the floor.
		Top right: Petitcat's egocentric memory. Black segments: black tape detection events. 
		Bottom right: phenomenon-centric memory. 
		Black hexagon: the point phenomenon used as origin of the allocentric reference frame. 
		Yellow hexagons: echo measured with the sonar. Red hexagon: focus of attention.} \label{fig:video}
\end{figure}

The C++ software running on the robot's Arduino board controls the enaction of primitive interactions. 
A personal computer implements the GEITH and the cognitive architecture that remote controls the robot via Wi-Fi.
The cognitive architecture selects the primitive interaction to try to enact and sends it to the robot. 
The robot tries to enact it and sends the outcome back to the PC.  
The code is open source and shared online \cite{petitcat_github}.

For this experiment, we defined four possible commands: \texttt{forward}, \texttt{backward}, \texttt{swipe}, and \texttt{turn}. 
\texttt{Forward} and \texttt{backward} are longitudinal translations. Their spatio-temporal attribute is the target duration (float).   
\texttt{Swipe} is a lateral translation. Its spatio-temporal attributes are the direction (left or right) and target duration (float). 
\texttt{Turn} consists in turning in place. Its spatio-temporal attribute is the target yaw (float), negative when counter-trigonometric. 
%they have been separated rather than using a direction attribute because they are conceptually different: moving toward or away from a target. 

The control loop monitors the elapsed time, yaw, and floor luminosity. %, and the distance of echo signals during the enaction of interactions. 
The termination conditions are reaching the target duration or yaw, or detecting the black tape, making two possible outcomes: \texttt{white} or \texttt{black}.
This gives eight primitive interactions identified by their tuple $\langle$command, outcome$\rangle$: 4 commands $\times$ 2 outcomes.
% Note that Petitcat cannot ``see'' the tape but only ``feel'' it through its floor luminosity sensors when he passes over it.
All interactions are given a zero prior preference except $\langle$\texttt{forward}, \texttt{white}$\rangle$ which has a positive one.
Additionally, the robot returns the measured spatio-temporal attributes: measured duration (float), measured yaw (float), and black tape detection (none, left, front, right). 

When the black tape is detected, the movement is interrupted and a ``reflex'' movement is performed to withdraw from the tape by a few centimeters. 
When detection is on the side, this withdrawal includes a rotation to the opposite side, which tends to bring the robot back into a position perpendicular to the tape. This behavior was implemented to prevent the robot from falling off a table or exiting the arena.

We seeded the cognitive architecture with the four composite interactions below, which constitute ``innate'' ways for Petitcat to interact with points. 
The GEITH tries to simulate them, computes their spatio-temporal attributes according to the position of the phenomenon in memory, and proposes those that are feasible in the current context.
\begin{enumerate}
	\item $\langle\langle$\texttt{forward}, \texttt{black}$\rangle\rangle$
	\item $\langle\langle$\texttt{turn}, \texttt{white}$\rangle$, $\langle$\texttt{forward}, \texttt{black}$\rangle\rangle$
	\item $\langle\langle$\texttt{swipe}, \texttt{white}$\rangle$, $\langle$\texttt{forward}, \texttt{black}$\rangle\rangle$
	\item $\langle\langle$\texttt{swipe}, \texttt{white}$\rangle$, $\langle$\texttt{turn}, \texttt{white}$\rangle$, $\langle$\texttt{forward}, \texttt{black}$\rangle\rangle$
\end{enumerate}

Neurotransmitter levels can vary from 0 to 100 and are initialized at 50. DA prevails in case of equality.
The prevalence of DA makes Petitcat initially select the $\langle$\texttt{forward}, \texttt{white}$\rangle$ interaction because it has a positive prior preference.
When he detects a point (by surprise), 5-HT increases to its max. 
The prevalence of 5-HT and the presence of a point phenomenon in memory trigger the selection of innate interactions with the point.
If the prediction errors do not decrease (that is, the prediction is not longer improving), 5-HT decreases.
When 5-HT drops below or equal to DA, the $\langle$\texttt{forward}, \texttt{white}$\rangle$ interaction is again selected, causing Petitcat to explore new destinations. 

Prediction errors may concern both the outcome of primitive interactions and the spatio-temporal measures.
Prediction errors on the outcome (\texttt{black} predicted but \texttt{white} occurred, or the reverse) mean that the selected primitive interaction failed and another interaction was actually enacted instead. 
Failing primitive interactions cause the composite interaction to which they belong to abort, and NA to rise to its max.

The GEITH uses a \textit{focus of attention} point and a \textit{prompt} point to compute the spatio-temporal attributes of interactions (Fig. \ref{fig:geith}, center). 
When Petitcat interacts with a point, the GEITH places the focus of attention at the place of the phenomenon. 
A failure to interact with the point means that the localization of the phenomenon in memory is erroneous. 
%generally due to drift caused by inaccuracy in the GEITH parameters and the spatio-temporal measures. 
The high NA level that occurs in case of failure causes the GEITH to move the focus of attention to another cell in phenomenon-centric memory in search of the point.
Cells compete to catch the focus with preference given to those closer to the last detected position of the phenomenon but having gone the longest period since last being visited.
The interactions with the point continue afterward based on the focus in different cells. 
NA is reset to 50 if Petitcat finds the lost point; otherwise, it progressively decreases until it drops below 50 causing Petitcat to abandon the search. 

In addition to the number of failed interactions, we also expect the prediction errors of the yaw and of the forward duration to decrease as the GEITH adjusts its parameters. 
Our GEITH has about 20 parameters, but this experiment only involves \texttt{FLOOR\_SENSOR\_POSITION}, \texttt{FORWARD\_SPEED}, \texttt{WITHDRAWAL\_DIS\-TANCE}, and \texttt{SIDE\_WITH\-DRA\-WAL\_YAW}.
Note that GEITH has no means to infer the absolute values of these parameters but can only adjust them in relation to each other.
The GEITH cannot either predict that the point will be detected on the side of the floor luminosity sensor, which will cause a withdrawal with rotation. 
The cognitive architecture makes the robot aim straight at the point.
The GEITH thus always predicts a straight withdrawal. 

%Ideally, we would like the GEITH to find by itself the relations of causality that link parameters to prediction errors, but in this experiment, we predefined these relations.


\section{Results}
\label{sec:results}

Several videos of experiment runs are available online \cite{georgeon_petitcat_playlist}. 
Here we analyze the representative run recorded in \cite{georgeon_petitcat_2024}.
In this run, Petitcat encountered the point in Step 1 and interacted with it up to Step 60.
In Step 17, it missed the point, but found it again in Step 20.
This is shown in the \textit{outcome code prediction error plot} in Fig. \ref{fig:outcome}. 
The fact that Peticat did not miss the point after Step 20 shows an improvement of the GEITH parameters.

\begin{figure}
	\includegraphics[width=\textwidth]{01_Outcome_code.pdf}
	\caption{Outcome prediction error plot: 0 if predicted outcome = actual outcome (successful interaction), 1 otherwise (failed interaction).
	Step 0: Petitcat moved forward.
	Step 1: he unexpectedly detected the point.
	Step 17: he expected to detect the point while translating forward but missed it.
	Step 18: he expected to not detect the point while turning but detected it.
	Step 20: he did not predict detecting the point but did. 
	Step 63: As he moved away from the point, he did not expect to detect the arena border. } \label{fig:outcome}
\end{figure}

As explained above, the GEITH cannot predict when Peticat will detect the point on the side. 
This can cause large yaw prediction errors because the robot unexpectedly turned during withdrawal.
Fig. \ref{fig:yaw_pe} shows these prediction errors that do not improve over time. 

\begin{figure}
	\includegraphics[width=\textwidth]{02_yaw_pe.pdf}
	\caption{Yaw prediction error plot. The prediction errors come from different causes which makes the interpretation of the plot difficult. 
		Points above 20 or below -20 are large prediction errors occurring when Petitcat turned while withdrawing because the GEITH did not predict detecting the point on the side.
	%	Step 2: Petitcat detected the point on the side which caused an unpredicted yaw to the left.   shows no improvement when we do not distinguish between the different kinds of interactions.
	} \label{fig:yaw_pe}
\end{figure}

The GEITH simulates turning while withdrawing based on the \texttt{SIDE\_WITH\-DRA\-WAL\_YAW} parameter.
To adjust this parameter, the GEITH must 
%isolate the interactions in which he detected the point on the side and 
compute the \textit{yaw residual error} 
%The yaw residual error is the residual prediction error 
that is left when knowing on which side the point was detected. 
Fig. \ref{fig:yaw_re} shows that the residual yaw error decreases as the robot adjusts the \texttt{SIDE\_WITH\-DRA\-WAL\_YAW} parameter.

\begin{figure}
	\includegraphics[width=\textwidth]{03_yaw_re.pdf}
	\caption{Yaw residual error of interactions that have a \texttt{black} outcome. 
		It shows a significant decrease as the robot interacts with the point.
	} \label{fig:yaw_re}
\end{figure}

The adjustment of \texttt{FORWARD\_SPEED} and \texttt{WITHDRAWAL\_DIS\-TANCE} allows for a visible decrease of the forward-duration prediction error shown in Fig. \ref{fig:forward_re}.

\begin{figure}
	\includegraphics[width=\textwidth]{07_Forward_duration_pe.pdf}
	\caption{Duration prediction error for \texttt{forward} interactions.
	Step 1 and 20: the forward translation was unexpectedly interrupted by the point detection.
	Step 17: the forward duration was longer than expected because the robot did not detect the point.
	Except for these events, the plot shows that the forward duration prediction error decreases as the robot is interacting with the point from Step 1 to 62.
	On Step 63: the robot moves away from the point and a forward duration prediction errors occur as he encounters a new object: the border of the arena.
	} \label{fig:forward_re}
\end{figure}

%\begin{figure}
%	\includegraphics[width=\textwidth]{04_Compass.pdf}
%	\caption{The compass residual error decreases after step 20 when the robot starts circling around the point.
%	It nonetheless remains noisy due to sensor imprecision.
%	The sliding average over 10 interactions tends to 0.8° and the standard deviation to 7.4°.} \label{fig:compass}
%\end{figure}

\section{Conclusion}

We demonstrated a simple robot that managed to reduce intuitive-physics prediction errors in an open environment. 
We drew inspiration from theories positing core knowledge in the brain that have innate origins.
Compared with our previous studies base on the same robot and earlier iterations of the cognitive architecture \cite{georgeon_artificial_2024}, the present study adds the model of emotion, the GEITH, and a set of innate behaviors.
These elements are hard-coded in the robot, but what is not predefined is the set of world states and the ontology of objects in the world.

%Compared with our previous studies base on the same robot and earlier iterations of the cognitive architecture, the present study implements the novel model of emotion and GEITH. 

When the robot finds an object, it instantiates a small local model in the reference frame of this object. 
This finite discrete model lends itself to regular active inference techniques. 
We continue studying how to optimize the process of GEITH refinement in such local models using the active inference python library  \texttt{inferac\-tively-pymdp} \cite{Heins2022}.
This approach, however, remains dependent on the causal structure of the GEITH itself.
How the robot could improve the causal structure of the GEITH or find exceptions remains an open question related to explainable AI \cite{thorisson_explanation_2021}.

%merely begins to explore the intricacies involved in developing and maintaining a causal model of perception.
This study illustrates Whitehead's two modes of perception. 
Mode 1, presentational immediacy (in the sense of ``seeing a color''), corresponds to the robot's boolean sensory input we label \texttt{white} and \texttt{black}. 
Mode 2, causal efficiency (in the sense of ``seeing an object''), corresponds to the robot maintaining a causal model of the point on the floor. 
We expect the next step to involve \textit{ mode-2 perception} of lines between points, which could open the way to learning the compositionality of phenomena. 
We shall also examine how the robot can perceive the appearance, disappearance, and displacement of objects. 

Beyond studying perception, we follow a method that advances through analogies between robots and natural organisms, aiming to enhance our understanding of the potential becoming of agency in robots. 
This method relates to the method of \textit{transduction} proposed by another philosopher of process philosophy, Gilbert Simondon \cite{simondon_individuation_2005}.
We try to design robots that mimic traits of natural organisms such as perception, surprise, emotions, and preferences.
We are not claiming that the robot can actually \textit{experience} these traits of agency, let alone have sentience. 
The robot, nonetheless, generates behaviors that human observers easily interpret as lifelike, which could find applications in companion robotics.
The robot seems to enjoy exploring for the mere pleasure of movement as it lights up in green (DA prevails); 
it plays with the point as it does practicing its skills as it lights up in white (5-HT prevails); 
it seems anxious to search for the lost point as it lights up in red (NA prevails). 
This interpretation is also reinforced by seeing that the robot also turns its head in search of objects around the arena.
In future experiments, we would like to study more precisely to what extent observers assign these subjective traits to the robot.
%It wish to further investigate how human observers project these interpretations in a controlled experiment. 





\begin{credits}

\subsubsection{\ackname}
Dr. de Montera acknowledges support by ANR under contract ANR-11-DPBS-0001.
Dr. Robertson acknowledges that this material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), USA under Contract No. HR001120C0035.

%A bold run-in heading in small font size at the end of the paper is
%used for general acknowledgments, for example: This study was funded
%by X (grant number Y).

%\subsubsection{\discintname}
%It is now necessary to declare any competing interests or to specifically
%state that the authors have no competing interests. Please place the
%statement with a bold run-in heading in small font size beneath the
%(optional) acknowledgments\footnote{If EquinOCS, our proceedings submission
%system, is used, then the disclaimer can be provided directly in the system.},
%for example: The authors have no competing interests to declare that are
%relevant to the content of this article. Or: Author A has received research
%grants from Company W. Author B has received a speaker honorarium from
%Company X and owns stock in Company Y. Author C is a member of committee Z.
\end{credits}
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{georgeon.bib}
%
\end{document}
